*Machine Learning*
  - Course:   COMP 4432 1,  COMP 3432 1, 410
  - Instructor: [[https://sites.google.com/site/poorannegi/][Pooran Singh Negi]], pooran.negi@gmail.com office 470, Office Hours:  M, Wed,  1.00 - 3 p.m. Email for 1-on-1 help.
  - (TA: Aiman Gannous (Aiman.Gannous@du.edu), Office 126)
 
* Prerequisite
Student should be comfortable with linear algebra, probability, statistics,
optimization and  programming experience in python and its scientifc libraries. We will cover some basics in the class.
-  [[http://cs229.stanford.edu/section/cs229-linalg.pdf][linear algebra overview]] 
-  Read chapter 2 of Kevin Murphy for probabilty and statistics review or any other text you have used in the past
* TextBooks
- *Required:*
  -  [[https://www.cs.ubc.ca/~murphyk/MLbook/][Machine Learning: a Probabilistic Perspective by Kevin Patrick Murphy]]. Online version is available in the [[https://library.du.edu/][DU library]].
- *Optional:*  [[http://www-bcf.usc.edu/~gareth/ISL/][An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani]]. It is available online in pdf format
- *Optional:*  [[http://www.deeplearningbook.org/][Deep learning]]  by Ian Goodfellow and Yoshua Bengio and Aaron Courville.   It is available online.
- *Optional:* For more advance treatment [[https://web.stanford.edu/~hastie/ElemStatLearn/][The Elements of Statistical Learning]]. It is available online in pdf format   
* Course Description
We will go though theory behind
machine learning using tool from probability and linear algebra.
We will use python, its [[https://www.scipy.org/][scientific libraries]] (numpy, scipy, matplotlib, Pandas etc.)
and [[http://scikit-learn.org/stable/][scikit-learn: Machine Learning in Python]] during the course. For deep neural network part, we will use
highly popular [[https://www.tensorflow.org/][tensorflow Machine Intelligence]] library from the Google. For assignments, starter code  or hint will be given. 
At the end of the course one should be able to understand theory behind various
machine learning algorithms with a unifying perspective from probability theory, be comfortable using open source tools for building machine learning systems.

* Software
Please install [[https://www.anaconda.com/download/][Anaconda for Python 3.6 version data science platform. ]]Please install it before coming in the class on Tuesday.
See the youtube link [[https://www.youtube.com/watch?v=OOFONKvaz0A][Installing Anaconda, Jupyter Notebook]]. 
You can also go to my  [[https://github.com/psnegi/PythonForReproducibleResearch][python for reproducible research]]  github repository and start by running pythonBasic.ipynb notebook.
I will go over basic of python and jupyter notebook. For Deep Neural networks, we will go over tensorlfow and keras installation instruction later in the course.
** Tensorflow and Keras
 -  tensorflow related information is [[./tensorflow.org][here]]
 -  Keras related information is [[https://keras.io/][here]]

** Python learning resources
   - [[https://try.jupyter.org/][try python notebook online without installing anything]]
   - [[http://pythontutor.com/live.html#mode%3Dedit][Runs and visualizes your python code]]
   - [[https://docs.python.org/3/tutorial/index.html][The Python Tutorial]]  
* Syllabus
*This syllabus is subject to change at the discretion of the instructor*
Here are the main topics for the class. More topics can be added as per class interest and available time.
- Basic idea of machine learning, and probability
- Generative models, parametric estimation and supervised learning.
  - Naive Bayes classifier etc.
- Gaussian models
- Linear and logistic regression
- Support vector machine, Kernels
- Matrix factorization
- Decision tree.
- Bagging, boosting and model selection etc.
- Unsupervised learning
  - Clustering etc.
- Deep learning
  - Artificial Neural Networks(ANN), End to end learning, cost function
  - Convolutional Neural Networks(CNN) for classification(image) and regression
  - Recurrent Neural Networks for natural language processing(NLP) and time series data
  - Generative adversarial networks (GANs) 

* Grading
There will be one mid term, a final exam, homework assignments, in class quizzes.
Worst 2 of your homework assignments and 2 quizzes will be dropped. Students enrolled in 
 COMP 4432 1,  will be asked to do slightly more homework questions.


|------------------------------------------------------------------+-------------|
| Homework + Quizzes                                               | 30(20+10) % |
|------------------------------------------------------------------+-------------|
| Midterm exam,  24 th July, in class                              |         20% |
|------------------------------------------------------------------+-------------|
| Final exam, 20 August, in class                                  |         25% |
|------------------------------------------------------------------+-------------|
| Final Project, 17 August 11.59 p.m                               |         15% |
|                                                                  |             |
|------------------------------------------------------------------+-------------|
| 10 minutes final presentation about project , 16 August in class |         10% |
|------------------------------------------------------------------+-------------|
|                                                                  |             |

grade range [('A', >=95), ('A_minus', >=90), ('B_plus', >=85), ('B', >=80), ('B_minus', >=75), ('C_plus', >=70), ('C', >=65), ('C_minus', >=60),
 ('D_plus', >=55), ('D', >=50), ('D_minus', >=45),  ('F', < 45)])

*Please respect DU [[https://www.du.edu/studentlife/studentconduct/honorcode.html][Honor Yourself, Honor the Code]]*

* Final Project
  Click [[./final_project.org][Here]] to see what is expected in final project
** Datsets for final Projects
  You can use any dataset you are interested in. Here is some listing of open datasets.
  - [[https://archive.ics.uci.edu/ml/datasets.html][UC Irvine Machine Learning Repository]]
  - [[https://www.kaggle.com/datasets][Kaggle Datasets]]  
  - [[https://github.com/niderhoff/nlp-datasets][nlp-datasets]]
  - [[https://data.worldbank.org/][World Bank Data]]
  - [[https://catalog.data.gov/dataset][U.S. Government's open data]]
  - [[https://www.census.gov/][United States Census Bureau]]
  - [[https://www.ncdc.noaa.gov/][National Climatic Data Center - NOAA]]
  - [[http://www.internationalgenome.org/data][IGSR: The International Genome Sample Resource]]


* Quiz

* Midterm
| Midterm | solution |
|---------+----------|
|       1 |          |
|---------+----------|

* Homework
Homework numbers are as per *Kevin Murphy ebook from the library*
| HW | Due Date           |                                                                                                         | sol |
|----+--------------------+---------------------------------------------------------------------------------------------------------+-----|
|    |                    |                                                                                                         |     |
| 1a | 29 June 11.59 p.m  | *coding part*:  [[file:hws/hw1_python_numpy_matplotlib.ipynb][python_numpy_matplotlib]]                                                                 |     |
|----+--------------------+---------------------------------------------------------------------------------------------------------+-----|
| 1b |                    | *written part*: Problem numbers are from kevin murphy book. *Use DU  library version*.                  |     |
|    | 28 June in class   | submit written solution: Chapter 2, 2.1(use bayes rule, condition on event actually observed.           |     |
|    |                    | like in part a say N_b = number of boys, N_g no of girls) (4 = 2+2 point), 2.3 (2 point), 2.4(1 point), |     |
|    |                    | 2.6(6 = 3+3 point), 2.12(2 point), 2.16(3= 1+1+1 points)                                                |     |
|    |                    | *Look for chapter 2 for definitions and explain various steps in the work*                              |     |
|----+--------------------+---------------------------------------------------------------------------------------------------------+-----|
| 2a | 5 th July in class | Chpater 2,    2.13                                                                                      |     |
|    |                    | chapter 3,    3.6, 3.7, 3.11, 3.20,                                                                     |     |
|----+--------------------+---------------------------------------------------------------------------------------------------------+-----|
|    |                    |                                                                                                         |     |

* Course announcements
|--------+--------------------------------------------------------------------------------|
| Date   | Announcement                                                                   |
|--------+--------------------------------------------------------------------------------|


* Course Lectures


| Date    | Reading assignment                                                                          | uploaded slides/notebooks       |
|---------+---------------------------------------------------------------------------------------------+---------------------------------|
| 19 June | Read chapter 1 of Kevin Murphy and Basic of probabilty from chapter 2 upto 2.4.1 and 2.4.6  |                                 |
|         | Detail [[https://www.scipy-lectures.org/][Scipy Lecture Notes]] . Practice 1.3.1 and 1.3.2, 1.4.1 to 1.4.2.8 in Jupyter notebook | [[./lectures/lecture1_19june/ml_motivation.ipynb][what is ml? why we care ?]]       |
|         |                                                                                             | [[lectures/lecture1_19june/numpy_basics.ipynb][python and numpy basics]]         |
|         |                                                                                             |                                 |
|---------+---------------------------------------------------------------------------------------------+---------------------------------|
| 21 June | section 2.2, 2.3, 2.4[.1, .2, .3, .4, .5, .6], 2.5[.1, .2, .4], 2.6.1, 2.8 of kevin Murphy  | [[lectures/lecture2_21june/Generative_model.ipynb][notebook generative model]]       |
|         | 3.1-3.2.4                                                                                   |                                 |
|---------+---------------------------------------------------------------------------------------------+---------------------------------|
| 26 June | Rest of chapter 3                                                                           | [[./lectures/lecture3_25june/note_probabilyt_and_information_theory.ipynb][information theory, mle and map]] |
|         |                                                                                             |                                 |
|---------+---------------------------------------------------------------------------------------------+---------------------------------|
| 28 June | NBC Naive Bayes classifiers                                                                 |                                 |

